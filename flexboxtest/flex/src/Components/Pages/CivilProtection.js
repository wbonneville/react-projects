import React, { Component } from "react";
import styled from "styled-components";

const Div = styled.div`
  font-size: 23px;
  margin-left: 7%;
  margin-right: 7%;
  margin-top: 4%;
  margin-bottom: 5%;

  & h4 {
    font-weight: 100;
    margin-top: -2%;
  }

  & .bigh1 {
    color: #ff6666;
    font-size: 22px;
    font-weight: 100;
  }
`;

export default class Housing extends Component {
  render() {
    return (
      <Div className="row start-xs">
        <div className="col-xs-10">
          <h1 className="bigh1">Messages from our Benefactors</h1>

          <h1>On Civil Protection</h1>

          <h4>September 16th, 2019</h4>
          <p>
            Some people think that artificial intelligence, particularly machine
            learning, has the potential to solve major problems such as
            unemployment, as well as to save lives, with health care, medical
            imaging, even traffic safety. Algorithms will spare us from a great
            many decisions that would otherwise have to be made. In addition,
            all sorts of things, from politics to social policy, will be made
            more efficient by artificial intelligence. But the downside of AI is
            even bigger: In addition to shrinking available jobs, AI will
            probably increase inequality in many countries, and enrich the
            already wealthy at the expense of the rest of us.
            <br></br>
            <br></br>
            It is a question, then, about whether certain types of AI will work
            well, whether they should work well, and how to prevent them from
            working well. One way to do that is to think about AI from the
            perspective of a great many people: the fully developed human mind.
            Let’s take a couple of cases. In the first case, there is a
            self-driving car. In the second case, there is a direct assault on
            the human brain, and probably other kinds of technology that make
            physical things too expensive to use for the same purpose. Based on
            the past century of experience with things like driverless cars,
            machines will learn more and more sophisticated functions. The
            brain, then, has a huge head start in knowing what types of tasks
            are valuable and what types are not, and in figuring out the best
            way to carry out those tasks — the well-suited, efficient way.
            <br></br>
            <br></br>
            The future of this industry is going to be determined by human
            competency. We can think about what we do well: It’s not work that
            we simply decide to do — it’s work that requires specific skills and
            abilities, like reading, like reading, like reading, like reading. A
            fully developed human mind, the type that the new generation of
            machines should be trying to emulate, can understand this
            relationship, and so may be able to tell the difference between
            expensive and cheap task types, and between tasks and things. In the
            case of self-driving cars, they could copy the intuitive and
            systematic structure of a fully developed human mind.
            <br></br>
            <br></br>
            But they might not, since, as they learn to perform tasks, they
            might develop error rates that are too high, or too low, for their
            purely robotic intelligence to pick up — perhaps in large part
            because of the nature of the tasks they are trying to do. This could
            have serious consequences. We might ask ourselves whether such a
            situation will be tolerable. Human beings, in many ways, and
            especially in terms of personal health care, have long lived with
            serious health problems, sometimes life-threatening ones. Could we
            live that way if we relied only on machines for everything? In some
            cases, we probably could, but it would be sad. There are, however,
            situations in which human competency is of less use to machines,
            because even the human brain is not absolutely flawless. For
            example, we can only identify trouble with the eye very well if we
            have good vision.
            <br></br>
            <br></br>
            There is a huge gulf between what robots can do and what the human
            mind can do, for any number of reasons. That leads to a second
            problem, which I’ll call “distortions.” In the world of science
            fiction, these can be deadly, often reducing whole populations to
            disaster. In other cases, they are more prosaic — like the use of AI
            by dictators to suppress political movements. I’m not really worried
            about the development of distorting devices. Distortions happen in
            any human or machine context, but at the moment only governments and
            journalists are interested in developing them, because that’s where
            their sources of data are. But it’s an open question whether there
            is as yet nothing terribly distorting about digital technologies.
            And once again, much of the world will be closely watching the US.
            <br></br>
            <br></br>
            The US is a major researcher and developer of AI — but we are not
            quite there yet, so we don’t have the experience of living with
            distortions. So when, some day, those distortions appear, they could
            show us how dangerous — or how useful — distorting technology can
            be. In other words, we need to be watchful, but there should be room
            for self-insurance too.
          </p>

          <h1>Managing AI</h1>
          <h4>September 16th, 2019</h4>
          <p>
            Our Civil Protection person, myself, and his team have been having
            this discussion a lot lately, and people are interested, especially
            with the recent construction of the security cams across Towns and
            the new defense installations, for example: Shouldn’t we use new
            technologies, like artificial intelligence, to help our people, and
            not to control them?
            <br></br>
            <br></br>
            In other words, AI wants to say, “Hello! All the people of this
            country are our problem. Could we stop them from harming each
            other?” As a civil protection manager you have a clear
            responsibility to ensure safety. I believe that it would make sense
            to look at AI for civil protection against major emergencies in
            order to adapt the system and build a stronger relationship with our
            public, perhaps including an AI CPO, perhaps a GP, maybe even an AI
            central command center where we can deploy our resources more
            efficiently and effectively.
            <br></br>
            <br></br>
            These ideas have been drawn up for the first time, with the help of
            ECITiap. We will try to find the best way to optimize our workforce
            to meet the new demands that are coming. I think that it is very
            important to know that, at this stage, we do not know yet how or
            whether to use AI, whether we will have a central command center
            with AI or only automatically detecting fires and floods, or an AI
            following the instructions of any of our government ministers, or
            anything else. We really need to open up new possibilities for all
            our civil protection work, without trying to control people. As a
            manager, it is important to know all our human resources and see how
            we can best organize them to achieve the objectives. This will allow
            us to increase efficiency and enhance safety and to improve how we
            deal with people and communities.
          </p>
        </div>
      </Div>
    );
  }
}
